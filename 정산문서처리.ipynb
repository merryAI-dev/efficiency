{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZ4-ZQ3EGSe5"
   },
   "source": [
    "# ğŸ“„ ì •ì‚° ë¬¸ì„œ ìë™ ì²˜ë¦¬ ì‹œìŠ¤í…œ\n",
    "\n",
    "## ê¸°ëŠ¥\n",
    "1. í´ë”ë³„ PDF ìŠ¤ìº” ë° ë¶„ë¥˜ (ê·œì¹™ ê¸°ë°˜)\n",
    "2. ë¬¸ì„œ ìˆœì„œëŒ€ë¡œ PDF í•©ë³¸ ìƒì„±\n",
    "3. í•„ìˆ˜ ë¬¸ì„œ ëˆ„ë½ ì²´í¬\n",
    "4. êµ¬ê¸€ ì‹œíŠ¸ìš© ê²°ê³¼ ë°ì´í„° ìƒì„±\n",
    "\n",
    "## ì‚¬ìš©ë²•\n",
    "ì…€ì„ ìœ„ì—ì„œë¶€í„° ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rvVjRSvGSe8"
   },
   "source": [
    "## 1ë‹¨ê³„: í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCb_yWanGSe9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761716024791,
     "user_tz": -540,
     "elapsed": 2798,
     "user": {
      "displayName": "ë³€ë¯¼ìš±(ë³´ëŒ)",
      "userId": "00628206658453677099"
     }
    },
    "outputId": "9c72e9ff-445c-42ae-d7fd-314696397f09"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8C9OqeHXGSe-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761716029304,
     "user_tz": -540,
     "elapsed": 4510,
     "user": {
      "displayName": "ë³€ë¯¼ìš±(ë³´ëŒ)",
      "userId": "00628206658453677099"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "daf15257-3960-475e-cf8b-77023c5cd820"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install PyPDF2 Pillow openai -q\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9_Ddb3IGSe-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761716029313,
     "user_tz": -540,
     "elapsed": 6,
     "user": {
      "displayName": "ë³€ë¯¼ìš±(ë³´ëŒ)",
      "userId": "00628206658453677099"
     }
    },
    "outputId": "dd313309-1d0d-443b-b8de-02a3c0ec1459"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from PyPDF2 import PdfMerger, PdfReader\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IlQiEGrGSe_"
   },
   "source": [
    "## 2ë‹¨ê³„: ì„¤ì • (ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1yKWN7mGSe_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761716029333,
     "user_tz": -540,
     "elapsed": 18,
     "user": {
      "displayName": "ë³€ë¯¼ìš±(ë³´ëŒ)",
      "userId": "00628206658453677099"
     }
    },
    "outputId": "fe071082-9ed4-4656-83b7-e4adb710231d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… ì„¤ì • ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# âš™ï¸ ì„¤ì • (ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”!)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"ì¤‘ì•™ ì§‘ì¤‘ì‹ ì„¤ì • ê´€ë¦¬\"\"\"\n",
    "\n",
    "    # ğŸ“ ê²½ë¡œ ì„¤ì •\n",
    "    BASE_PATH = \"ê²½ë¡œ ì…ë ¥\"\n",
    "\n",
    "    # ğŸ¤– OpenAI API ì„¤ì •\n",
    "    OPENAI_API_KEY = \"KEY ì…ë ¥\"  # â¬…ï¸ ì—¬ê¸°ì— OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”!\n",
    "    USE_GPT_CLASSIFICATION = True  # True = GPT ì‚¬ìš©, False = ì •ê·œì‹ ì‚¬ìš©\n",
    "\n",
    "    # ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ\n",
    "    TEST_MODE = False  # False = ì „ì²´ í´ë” ì²˜ë¦¬\n",
    "\n",
    "    # ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì„¤ì •\n",
    "    SHEET_URL = \"https://docs.google.com/spreadsheets/d/YOUR_SHEET_ID/edit\"  # êµ¬ê¸€ ì‹œíŠ¸ URL ì…ë ¥\n",
    "\n",
    "    # ğŸ”§ ì²˜ë¦¬ ì˜µì…˜\n",
    "    SKIP_EXISTING_MERGED = True\n",
    "    CLEANUP_TEMP_FILES = True\n",
    "\n",
    "    # ğŸ“ í•„ìˆ˜ ë¬¸ì„œ ì •ì˜ (ì‹¤ì œ í•„ìˆ˜ ì„œë¥˜ ëª©ë¡)\n",
    "    REQUIRED_DOCS = {\n",
    "        \"ìš©ì—­ë¹„\": [\"ì „ìì„¸ê¸ˆê³„ì‚°ì„œ\", \"ê²¬ì ì„œ\", \"ê³„ì•½ì„œ\", \"ì´ì²´í™•ì¸ì¦\", \"ì‚¬ì—…ìë“±ë¡ì¦\", \"í†µì¥ì‚¬ë³¸\", \"ìë¬¸ë³´ê³ ì„œ\"],\n",
    "        \"ì‚¬ì—…ì¶”ì§„ë¹„\": [\"ë§¤ì¶œì „í‘œ\", \"ì˜ìˆ˜ì¦\", \"íšŒì˜ë¡\"],\n",
    "        \"ì¼ë°˜ìˆ˜ìš©ë¹„\": [\"ì›ì²œì§•ìˆ˜ì˜ìˆ˜ì¦\", \"ì´ì²´í™•ì¸ì¦\", \"ì´ë ¥ì„œ\", \"í†µì¥ì‚¬ë³¸\", \"ì‹ ë¶„ì¦\", \"ë¹„ìš©ì§€ê¸‰í™•ì¸ì„œ\", \"ìë¬¸ë³´ê³ ì„œ\"],\n",
    "        \"í–‰ì‚¬ë¹„\": [\"ë§¤ì¶œì „í‘œ\"],\n",
    "    }\n",
    "\n",
    "    # ğŸ“„ ë¬¸ì„œ ìˆœì„œ ì •ì˜ (í•©ë³¸ ì‹œ ì‚¬ìš©)\n",
    "    DOCUMENT_ORDER = {\n",
    "        \"ìš©ì—­ë¹„\": [\"ì „ìì„¸ê¸ˆê³„ì‚°ì„œ\", \"ê²¬ì ì„œ\", \"ê³„ì•½ì„œ\", \"ì´ì²´í™•ì¸ì¦\", \"ì‚¬ì—…ìë“±ë¡ì¦\", \"í†µì¥ì‚¬ë³¸\", \"ìë¬¸ë³´ê³ ì„œ\", \"ê¸°íƒ€\"],\n",
    "        \"ì‚¬ì—…ì¶”ì§„ë¹„\": [\"ë§¤ì¶œì „í‘œ\", \"ì˜ìˆ˜ì¦\", \"íšŒì˜ë¡\", \"ê¸°íƒ€\"],\n",
    "        \"ì¼ë°˜ìˆ˜ìš©ë¹„\": [\"ì›ì²œì§•ìˆ˜ì˜ìˆ˜ì¦\", \"ì´ì²´í™•ì¸ì¦\", \"ì´ë ¥ì„œ\", \"í†µì¥ì‚¬ë³¸\", \"ì‹ ë¶„ì¦\", \"ë¹„ìš©ì§€ê¸‰í™•ì¸ì„œ\", \"ìë¬¸ë³´ê³ ì„œ\", \"ê°•ì˜ìë£Œ\", \"ê¸°íƒ€\"],\n",
    "        \"í–‰ì‚¬ë¹„\": [\"ë§¤ì¶œì „í‘œ\", \"ê¸°íƒ€\"],\n",
    "    }\n",
    "\n",
    "print(\"âœ… ì„¤ì • ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tolbZ62jGSe_"
   },
   "source": [
    "## 3ë‹¨ê³„: í•µì‹¬ í´ë˜ìŠ¤ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class FileConverter:\n",
    "    \"\"\"ì´ë¯¸ì§€ íŒŒì¼ì„ PDFë¡œ ë³€í™˜\"\"\"\n",
    "\n",
    "    SUPPORTED_IMAGES = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff'}\n",
    "\n",
    "    @classmethod\n",
    "    def is_image(cls, file_path: Path) -> bool:\n",
    "        \"\"\"ì´ë¯¸ì§€ íŒŒì¼ì¸ì§€ í™•ì¸\"\"\"\n",
    "        return file_path.suffix.lower() in cls.SUPPORTED_IMAGES\n",
    "\n",
    "    @classmethod\n",
    "    def convert_image_to_pdf(cls, image_path: Path, output_path: Path) -> bool:\n",
    "        \"\"\"\n",
    "        ì´ë¯¸ì§€ë¥¼ PDFë¡œ ë³€í™˜\n",
    "\n",
    "        Args:\n",
    "            image_path: ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ\n",
    "            output_path: ì¶œë ¥ PDF ê²½ë¡œ\n",
    "\n",
    "        Returns:\n",
    "            bool: ë³€í™˜ ì„±ê³µ ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # ì´ë¯¸ì§€ ì—´ê¸°\n",
    "            img = Image.open(image_path)\n",
    "\n",
    "            # RGBA ëª¨ë“œë¥¼ RGBë¡œ ë³€í™˜ (PDFëŠ” RGBA ë¯¸ì§€ì›)\n",
    "            if img.mode in ('RGBA', 'LA', 'P'):\n",
    "                # í°ìƒ‰ ë°°ê²½ ìƒì„±\n",
    "                background = Image.new('RGB', img.size, (255, 255, 255))\n",
    "                if img.mode == 'P':\n",
    "                    img = img.convert('RGBA')\n",
    "                background.paste(img, mask=img.split()[-1] if img.mode in ('RGBA', 'LA') else None)\n",
    "                img = background\n",
    "            elif img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "\n",
    "            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (A4 ë¹„ìœ¨ì— ë§ê²Œ)\n",
    "            # A4 = 210mm x 297mm = 2480px x 3508px at 300dpi\n",
    "            max_width, max_height = 2480, 3508\n",
    "            img.thumbnail((max_width, max_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "            # PDFë¡œ ì €ì¥\n",
    "            img.save(output_path, 'PDF', resolution=100.0, quality=95)\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸  ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨ ({image_path.name}): {str(e)}\")\n",
    "            return False\n",
    "\n",
    "print(\"âœ… FileConverter ì •ì˜ ì™„ë£Œ\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Uwkv12lyryG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761716029358,
     "user_tz": -540,
     "elapsed": 23,
     "user": {
      "displayName": "ë³€ë¯¼ìš±(ë³´ëŒ)",
      "userId": "00628206658453677099"
     }
    },
    "outputId": "3c26e2b9-fcad-42f1-f9c3-3a0f32f737d9"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… FileConverter ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ib3XrL0DGSfA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761716029451,
     "user_tz": -540,
     "elapsed": 91,
     "user": {
      "displayName": "ë³€ë¯¼ìš±(ë³´ëŒ)",
      "userId": "00628206658453677099"
     }
    },
    "outputId": "1e834622-1744-4094-a71e-cb1f99e45fd0"
   },
   "outputs": [],
   "source": [
    "# GPT ê¸°ë°˜ ë¬¸ì„œ ì¶”ì¶œ ë° ë¶„ë¥˜\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "class GPTDocumentExtractor:\n",
    "    \"\"\"GPT-4o-mini ê¸°ë°˜ ë¬¸ì„œ ì¶”ì¶œ ë° ë¶„ë¥˜\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = \"gpt-4o-mini\"\n",
    "        self.call_count = 0\n",
    "\n",
    "    def classify_file(self, filename: str) -> str:\n",
    "        \"\"\"íŒŒì¼ëª…ìœ¼ë¡œ ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ (GPT)\"\"\"\n",
    "        self.call_count += 1\n",
    "\n",
    "        prompt = f\"\"\"ë‹¤ìŒ íŒŒì¼ëª…ì„ ë³´ê³  ë¬¸ì„œ íƒ€ì…ì„ ë¶„ë¥˜í•˜ì„¸ìš”.\n",
    "\n",
    "íŒŒì¼ëª…: {filename}\n",
    "\n",
    "ê°€ëŠ¥í•œ ë¬¸ì„œ íƒ€ì…:\n",
    "- ì´ì²´í™•ì¸ì¦\n",
    "- ì „ìì„¸ê¸ˆê³„ì‚°ì„œ\n",
    "- ì›ì²œì§•ìˆ˜ì˜ìˆ˜ì¦\n",
    "- ë§¤ì¶œì „í‘œ\n",
    "- ì‚¬ì—…ìë“±ë¡ì¦\n",
    "- ê³„ì•½ì„œ\n",
    "- ê²¬ì ì„œ\n",
    "- ì‹ ë¶„ì¦\n",
    "- í†µì¥ì‚¬ë³¸\n",
    "- ì´ë ¥ì„œ\n",
    "- ë¹„ìš©ì§€ê¸‰í™•ì¸ì„œ\n",
    "- ìë¬¸ë³´ê³ ì„œ\n",
    "- ê°•ì˜ìë£Œ\n",
    "- íšŒì˜ë¡\n",
    "- ë°©ëª…ë¡\n",
    "- ì˜ìˆ˜ì¦\n",
    "- ê¸°íƒ€\n",
    "\n",
    "ìœ„ ëª©ë¡ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª… ë¶ˆí•„ìš”.\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"ì •ì‚° ë¬¸ì„œ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=30\n",
    "            )\n",
    "\n",
    "            result = response.choices[0].message.content.strip()\n",
    "\n",
    "            valid_types = {\n",
    "                'ì´ì²´í™•ì¸ì¦', 'ì „ìì„¸ê¸ˆê³„ì‚°ì„œ', 'ì›ì²œì§•ìˆ˜ì˜ìˆ˜ì¦', 'ë§¤ì¶œì „í‘œ',\n",
    "                'ì‚¬ì—…ìë“±ë¡ì¦', 'ê³„ì•½ì„œ', 'ê²¬ì ì„œ', 'ì‹ ë¶„ì¦', 'í†µì¥ì‚¬ë³¸',\n",
    "                'ì´ë ¥ì„œ', 'ë¹„ìš©ì§€ê¸‰í™•ì¸ì„œ', 'ìë¬¸ë³´ê³ ì„œ', 'ê°•ì˜ìë£Œ',\n",
    "                'íšŒì˜ë¡', 'ë°©ëª…ë¡', 'ì˜ìˆ˜ì¦', 'ê¸°íƒ€'\n",
    "            }\n",
    "\n",
    "            return result if result in valid_types else 'ê¸°íƒ€'\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ GPT íŒŒì¼ ë¶„ë¥˜ ì˜¤ë¥˜: {e}\")\n",
    "            return 'ê¸°íƒ€'\n",
    "\n",
    "    def extract_folder_info(self, folder_name: str) -> dict:\n",
    "        \"\"\"í´ë”ëª…ì—ì„œ ë²ˆí˜¸, ì´ë¦„, ë¹„ìš©ìœ í˜• ì¶”ì¶œ (GPT)\"\"\"\n",
    "        self.call_count += 1\n",
    "\n",
    "        prompt = f\"\"\"ë‹¤ìŒ í´ë”ëª…ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.\n",
    "\n",
    "í´ë”ëª…: {folder_name}\n",
    "\n",
    "ì¶”ì¶œí•  ì •ë³´:\n",
    "1. ë²ˆí˜¸ (ì˜ˆ: [\"149\"] ë˜ëŠ” [\"149\", \"152\"])\n",
    "2. ì—…ì²´/ê°œì¸ ì´ë¦„ (ì˜ˆ: \"ì´ì •ê·¼\", \"ë¬¸ì¹´ë°ë¯¸ì£¼ì‹íšŒì‚¬\", \"(ì£¼)ì¼€ì´í‹°ì•¤ì§€\")\n",
    "3. ë¹„ìš© ìœ í˜• (ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜: \"ì¼ë°˜ìˆ˜ìš©ë¹„\", \"ìš©ì—­ë¹„\", \"í–‰ì‚¬ë¹„\", \"ì‚¬ì—…ì¶”ì§„ë¹„\", \"êµ­ë‚´ì—¬ë¹„\", \"ê¸°íƒ€\")\n",
    "\n",
    "JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€:\n",
    "{{\n",
    "  \"numbers\": [\"149\"],\n",
    "  \"entity_name\": \"ì´ì •ê·¼\",\n",
    "  \"expense_type\": \"ì¼ë°˜ìˆ˜ìš©ë¹„\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"ì •ì‚° í´ë”ëª… ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=150\n",
    "            )\n",
    "\n",
    "            result = response.choices[0].message.content.strip()\n",
    "\n",
    "            # JSON íŒŒì‹±\n",
    "            if result.startswith(\"```json\"):\n",
    "                result = result[7:]\n",
    "            if result.startswith(\"```\"):\n",
    "                result = result[3:]\n",
    "            if result.endswith(\"```\"):\n",
    "                result = result[:-3]\n",
    "\n",
    "            data = json.loads(result.strip())\n",
    "\n",
    "            valid_expense_types = {\n",
    "                'ì¼ë°˜ìˆ˜ìš©ë¹„', 'ìš©ì—­ë¹„', 'í–‰ì‚¬ë¹„', 'ì‚¬ì—…ì¶”ì§„ë¹„', 'êµ­ë‚´ì—¬ë¹„', 'ê¸°íƒ€'\n",
    "            }\n",
    "\n",
    "            if data.get('expense_type') not in valid_expense_types:\n",
    "                data['expense_type'] = 'ê¸°íƒ€'\n",
    "\n",
    "            return data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ GPT í´ë” ë¶„ì„ ì˜¤ë¥˜: {e}\")\n",
    "            return {\n",
    "                \"numbers\": None,\n",
    "                \"entity_name\": None,\n",
    "                \"expense_type\": \"ê¸°íƒ€\"\n",
    "            }\n",
    "\n",
    "# GPT Extractor ì´ˆê¸°í™”\n",
    "if Config.USE_GPT_CLASSIFICATION:\n",
    "    gpt_extractor = GPTDocumentExtractor(Config.OPENAI_API_KEY)\n",
    "    print(\"âœ… GPT DocumentExtractor ì´ˆê¸°í™” ì™„ë£Œ (íŒŒì¼ + í´ë” ë¶„ë¥˜)\")\n",
    "    print(f\"   API í‚¤: {Config.OPENAI_API_KEY[:20]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì •ê·œì‹ ëª¨ë“œ (GPT ë¯¸ì‚¬ìš©)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class FolderScanner:\n",
    "    \"\"\"í´ë” ìŠ¤ìº” ë° ë¬¸ì„œ ë¶„ë¥˜ (GPT ì™„ì „ í†µí•©)\"\"\"\n",
    "\n",
    "    def __init__(self, base_path: str):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.converter = FileConverter()\n",
    "\n",
    "    def scan_all(self) -> List[Dict]:\n",
    "        \"\"\"ëª¨ë“  í´ë” ìŠ¤ìº”\"\"\"\n",
    "        results = []\n",
    "        for folder in self.base_path.iterdir():\n",
    "            if not folder.is_dir() or folder.name in ['ì£¼ë””ë„¤', '.claude']:\n",
    "                continue\n",
    "            folder_info = self.scan_folder(folder)\n",
    "            if folder_info:\n",
    "                results.append(folder_info)\n",
    "        return results\n",
    "\n",
    "    def scan_folder(self, folder_path: Path) -> Optional[Dict]:\n",
    "        \"\"\"í´ë” ìŠ¤ìº” ë° íŒŒì¼ ìˆ˜ì§‘ (GPTë¡œ ì „ë¶€ ì²˜ë¦¬)\"\"\"\n",
    "        folder_name = folder_path.name\n",
    "\n",
    "        # PDF íŒŒì¼ ìˆ˜ì§‘ (í•©ë³¸ ì œì™¸)\n",
    "        pdf_files = [p for p in folder_path.glob('*.pdf') if 'í•©ë³¸' not in p.name]\n",
    "\n",
    "        # ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘\n",
    "        image_files = []\n",
    "        for ext in FileConverter.SUPPORTED_IMAGES:\n",
    "            image_files.extend(folder_path.glob(f'*{ext}'))\n",
    "\n",
    "        if not pdf_files and not image_files:\n",
    "            return None\n",
    "\n",
    "        # ì„ì‹œ PDF ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "        temp_pdf_dir = None\n",
    "        if image_files:\n",
    "            temp_pdf_dir = folder_path / '.temp_pdfs'\n",
    "            temp_pdf_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # ë¬¸ì„œ ë¶„ë¥˜\n",
    "        documents = {}\n",
    "        all_files = []\n",
    "        conversion_log = []\n",
    "\n",
    "        # 1. PDF íŒŒì¼ ì²˜ë¦¬ (GPTë¡œ ë¶„ë¥˜)\n",
    "        for pdf in pdf_files:\n",
    "            if Config.USE_GPT_CLASSIFICATION:\n",
    "                doc_type = gpt_extractor.classify_file(pdf.name)\n",
    "            else:\n",
    "                # ì •ê·œì‹ í´ë°± (ì‚¬ìš© ì•ˆ í•¨)\n",
    "                doc_type = 'ê¸°íƒ€'\n",
    "\n",
    "            if doc_type not in documents:\n",
    "                documents[doc_type] = []\n",
    "            documents[doc_type].append({\n",
    "                'filename': pdf.name,\n",
    "                'path': str(pdf),\n",
    "                'original_path': str(pdf),\n",
    "                'is_converted': False\n",
    "            })\n",
    "            all_files.append(str(pdf))\n",
    "\n",
    "        # 2. ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ (GPTë¡œ ë¶„ë¥˜)\n",
    "        for img in image_files:\n",
    "            temp_pdf_name = f\"{img.stem}_converted.pdf\"\n",
    "            temp_pdf_path = temp_pdf_dir / temp_pdf_name\n",
    "\n",
    "            success = self.converter.convert_image_to_pdf(img, temp_pdf_path)\n",
    "\n",
    "            if success:\n",
    "                if Config.USE_GPT_CLASSIFICATION:\n",
    "                    doc_type = gpt_extractor.classify_file(img.name)\n",
    "                else:\n",
    "                    doc_type = 'ê¸°íƒ€'\n",
    "\n",
    "                if doc_type not in documents:\n",
    "                    documents[doc_type] = []\n",
    "                documents[doc_type].append({\n",
    "                    'filename': img.name,\n",
    "                    'path': str(temp_pdf_path),\n",
    "                    'original_path': str(img),\n",
    "                    'is_converted': True\n",
    "                })\n",
    "                all_files.append(str(temp_pdf_path))\n",
    "                conversion_log.append({'file': img.name, 'status': 'success', 'type': doc_type})\n",
    "            else:\n",
    "                conversion_log.append({'file': img.name, 'status': 'failed', 'type': 'unknown'})\n",
    "\n",
    "        # í´ë” ì •ë³´ ì¶”ì¶œ (GPT)\n",
    "        if Config.USE_GPT_CLASSIFICATION:\n",
    "            folder_info = gpt_extractor.extract_folder_info(folder_name)\n",
    "            numbers = folder_info.get('numbers')\n",
    "            entity_name = folder_info.get('entity_name')\n",
    "            expense_type = folder_info.get('expense_type')\n",
    "        else:\n",
    "            numbers = None\n",
    "            entity_name = None\n",
    "            expense_type = 'ê¸°íƒ€'\n",
    "\n",
    "        return {\n",
    "            'folder_name': folder_name,\n",
    "            'folder_path': str(folder_path),\n",
    "            'numbers': numbers,\n",
    "            'entity_name': entity_name,\n",
    "            'expense_type': expense_type,\n",
    "            'documents': documents,\n",
    "            'total_files': len(pdf_files) + len(image_files),\n",
    "            'pdf_files': all_files,\n",
    "            'conversion_log': conversion_log,\n",
    "            'temp_pdf_dir': str(temp_pdf_dir) if temp_pdf_dir else None\n",
    "        }\n",
    "\n",
    "    def get_summary(self, folder_info: Dict) -> str:\n",
    "        \"\"\"ë¬¸ì„œ ìš”ì•½ ì •ë³´ ìƒì„±\"\"\"\n",
    "        parts = [f\"{t}({len(fs)})\" for t, fs in sorted(folder_info['documents'].items())]\n",
    "        return \", \".join(parts)\n",
    "\n",
    "    def cleanup_temp_files(self, folder_info: Dict):\n",
    "        \"\"\"ì„ì‹œ ë³€í™˜ íŒŒì¼ ì •ë¦¬\"\"\"\n",
    "        if folder_info.get('temp_pdf_dir'):\n",
    "            temp_dir = Path(folder_info['temp_pdf_dir'])\n",
    "            if temp_dir.exists():\n",
    "                for f in temp_dir.glob('*_converted.pdf'):\n",
    "                    try:\n",
    "                        f.unlink()\n",
    "                    except:\n",
    "                        pass\n",
    "                try:\n",
    "                    temp_dir.rmdir()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "print(\"âœ… FolderScanner (GPT ì™„ì „ í†µí•©) ì •ì˜ ì™„ë£Œ\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFwJkzZdyryH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761716029463,
     "user_tz": -540,
     "elapsed": 10,
     "user": {
      "displayName": "ë³€ë¯¼ìš±(ë³´ëŒ)",
      "userId": "00628206658453677099"
     }
    },
    "outputId": "c8558657-5315-499a-8b15-51310d1a7e58"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… FolderScanner (GPT ì™„ì „ í†µí•©) ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class PDFMerger:\n",
    "    \"\"\"PDF í•©ë³¸ ìƒì„±\"\"\"\n",
    "\n",
    "    def sort_by_order(self, folder_info: dict, doc_order: dict) -> List[str]:\n",
    "        expense_type = folder_info['expense_type']\n",
    "        documents = folder_info['documents']\n",
    "        order = doc_order.get(expense_type, [])\n",
    "\n",
    "        if not order:\n",
    "            return folder_info['pdf_files']\n",
    "\n",
    "        sorted_files = []\n",
    "        for doc_type in order:\n",
    "            if doc_type in documents:\n",
    "                sorted_files.extend([f['path'] for f in documents[doc_type]])\n",
    "        return sorted_files\n",
    "\n",
    "    def merge(self, pdf_files: List[str], output_path: str) -> tuple:\n",
    "        \"\"\"PDF ë³‘í•© - ì†ìƒëœ íŒŒì¼ì€ ê±´ë„ˆëœ€\"\"\"\n",
    "        try:\n",
    "            merger = PdfMerger()\n",
    "            success = []\n",
    "            failed = []\n",
    "\n",
    "            for pdf_file in pdf_files:\n",
    "                try:\n",
    "                    reader = PdfReader(pdf_file)\n",
    "                    if len(reader.pages) > 0:\n",
    "                        merger.append(pdf_file)\n",
    "                        success.append(Path(pdf_file).name)\n",
    "                except Exception as e:\n",
    "                    failed.append(Path(pdf_file).name)\n",
    "\n",
    "            if len(merger.pages) > 0:\n",
    "                merger.write(output_path)\n",
    "                merger.close()\n",
    "                return (True, len(success), len(failed), len(merger.pages))\n",
    "            return (False, 0, len(failed), 0)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì˜¤ë¥˜: {e}\")\n",
    "            return (False, 0, len(pdf_files), 0)\n",
    "\n",
    "    def merge_folder(self, folder_info: dict, doc_order: dict = None) -> Optional[str]:\n",
    "        folder_path = Path(folder_info['folder_path'])\n",
    "\n",
    "        # ìˆœì„œëŒ€ë¡œ ì •ë ¬\n",
    "        pdf_files = self.sort_by_order(folder_info, doc_order) if doc_order else folder_info['pdf_files']\n",
    "\n",
    "        if not pdf_files:\n",
    "            return None\n",
    "\n",
    "        # íŒŒì¼ëª… ìƒì„±\n",
    "        numbers = folder_info.get('numbers', [])\n",
    "        entity = folder_info.get('entity_name', 'ì´ë¦„ì—†ìŒ')\n",
    "        expense = folder_info.get('expense_type', 'ê¸°íƒ€')\n",
    "        num_part = numbers[0] if numbers and len(numbers) == 1 else f\"{numbers[0]}-{numbers[-1]}\" if numbers else \"000\"\n",
    "\n",
    "        filename = f\"{num_part}_{entity}_{expense}_í•©ë³¸.pdf\"\n",
    "        for char in '<>:\"|?*':\n",
    "            filename = filename.replace(char, '_')\n",
    "\n",
    "        output_path = folder_path / filename\n",
    "\n",
    "        if output_path.exists():\n",
    "            print(f\"â„¹ï¸  ì´ë¯¸ ì¡´ì¬: {filename}\")\n",
    "            return str(output_path)\n",
    "\n",
    "        print(f\"\\nğŸ“ {folder_info['folder_name']}\")\n",
    "        success, success_cnt, failed_cnt, total_pages = self.merge(pdf_files, str(output_path))\n",
    "\n",
    "        if success:\n",
    "            print(f\"   âœ… {filename}\")\n",
    "            print(f\"   ì„±ê³µ: {success_cnt}/{len(pdf_files)}ê°œ íŒŒì¼, {total_pages} í˜ì´ì§€\")\n",
    "            if failed_cnt > 0:\n",
    "                print(f\"   âš ï¸  ì†ìƒëœ íŒŒì¼: {failed_cnt}ê°œ\")\n",
    "            return str(output_path)\n",
    "        else:\n",
    "            print(f\"   âŒ ì‹¤íŒ¨\")\n",
    "            return None\n",
    "\n",
    "print(\"âœ… PDFMerger ì •ì˜ ì™„ë£Œ\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0JRXjL-xyryI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761716029489,
     "user_tz": -540,
     "elapsed": 24,
     "user": {
      "displayName": "ë³€ë¯¼ìš±(ë³´ëŒ)",
      "userId": "00628206658453677099"
     }
    },
    "outputId": "93c06961-6989-4ff3-8f59-a4e8c660ed0b"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… PDFMerger ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4ë‹¨ê³„: ë©”ì¸ ì‹¤í–‰ â–¶ï¸\n",
    "\n",
    "**ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ëª¨ë“  ì²˜ë¦¬ê°€ ì§„í–‰ë©ë‹ˆë‹¤!**"
   ],
   "metadata": {
    "id": "0Y5JTa3tyryI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ë©”ì¸ ì‹¤í–‰ - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë¡œê·¸\n",
    "print(\"=\" * 80)\n",
    "print(\"ì •ì‚° ë¬¸ì„œ ìë™ ì²˜ë¦¬ ì‹œìŠ¤í…œ\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# 1. í´ë” ìŠ¤ìº”\n",
    "print(\"[1/3] í´ë” ìŠ¤ìº” ì¤‘...\")\n",
    "scanner = FolderScanner(Config.BASE_PATH)\n",
    "all_folders = scanner.scan_all()\n",
    "print(f\"      -> {len(all_folders)}ê°œ í´ë” ë°œê²¬\")\n",
    "print()\n",
    "\n",
    "# ìŠ¤ìº” ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 5ê°œ)\n",
    "print(\"ìŠ¤ìº” ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "for i, folder_info in enumerate(all_folders[:5], 1):\n",
    "    numbers = folder_info.get('numbers', [])\n",
    "    num_str = numbers[0] if numbers and len(numbers) == 1 else f\"{numbers[0]}-{numbers[-1]}\" if numbers else \"?\"\n",
    "    print(f\"  [{num_str}] {folder_info.get('entity_name', '?')} ({folder_info['expense_type']})\")\n",
    "if len(all_folders) > 5:\n",
    "    print(f\"  ... ì™¸ {len(all_folders)-5}ê°œ\")\n",
    "print()\n",
    "\n",
    "# 2. í•©ë³¸ ìƒì„± (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)\n",
    "print(f\"[2/3] PDF í•©ë³¸ ìƒì„± ì¤‘... (ì´ {len(all_folders)}ê°œ)\")\n",
    "print(\"-\" * 80)\n",
    "merger = PDFMerger()\n",
    "merged_results = []\n",
    "\n",
    "for i, folder_info in enumerate(all_folders, 1):\n",
    "    numbers = folder_info.get('numbers', [])\n",
    "    num_str = numbers[0] if numbers and len(numbers) == 1 else f\"{numbers[0]}-{numbers[-1]}\" if numbers else \"000\"\n",
    "    entity = folder_info.get('entity_name', '?')\n",
    "    expense = folder_info['expense_type']\n",
    "\n",
    "    print(f\"[{i}/{len(all_folders)}] {num_str}. {entity} ({expense})\", end=\" \")\n",
    "\n",
    "    result = merger.merge_folder(folder_info, Config.DOCUMENT_ORDER)\n",
    "    if result:\n",
    "        merged_results.append({\n",
    "            'folder': folder_info['folder_name'],\n",
    "            'merged_file': result\n",
    "        })\n",
    "        print(f\"âœ… {Path(result).name}\")\n",
    "    else:\n",
    "        print(\"âŒ ì‹¤íŒ¨\")\n",
    "\n",
    "    # ì„ì‹œ íŒŒì¼ ì •ë¦¬\n",
    "    if Config.CLEANUP_TEMP_FILES:\n",
    "        scanner.cleanup_temp_files(folder_info)\n",
    "\n",
    "print()\n",
    "print(f\"í•©ë³¸ ìƒì„± ì™„ë£Œ: {len(merged_results)}/{len(all_folders)}ê°œ ì„±ê³µ\")\n",
    "print()\n",
    "\n",
    "# 3. ê²°ê³¼ ë°ì´í„° ìƒì„±\n",
    "print(\"[3/3] ê²°ê³¼ ë°ì´í„° ìƒì„± ì¤‘...\")\n",
    "data = [[\"í´ë”ë²ˆí˜¸\", \"ì´ë¦„\", \"ë¹„ìš©ìœ í˜•\", \"ì—…ë¡œë“œíŒŒì¼\", \"í•„ìˆ˜ë¬¸ì„œì²´í¬\", \"ëˆ„ë½ë¬¸ì„œ\", \"í•©ë³¸íŒŒì¼ëª…\"]]\n",
    "\n",
    "for folder_info in all_folders:\n",
    "    numbers = folder_info.get('numbers', [])\n",
    "    num_str = numbers[0] if numbers and len(numbers) == 1 else f\"{numbers[0]}-{numbers[-1]}\" if numbers else \"ì—†ìŒ\"\n",
    "    entity = folder_info.get('entity_name', 'ì´ë¦„ì—†ìŒ')\n",
    "    expense_type = folder_info['expense_type']\n",
    "\n",
    "    uploaded = scanner.get_summary(folder_info)\n",
    "\n",
    "    required = Config.REQUIRED_DOCS.get(expense_type, [])\n",
    "    existing = list(folder_info['documents'].keys())\n",
    "    missing = [doc for doc in required if doc not in existing]\n",
    "\n",
    "    check = \"âœ…\" if not missing else \"âŒ\"\n",
    "    missing_str = \", \".join(missing) if missing else \"ì—†ìŒ\"\n",
    "\n",
    "    merged_file = next((m['merged_file'] for m in merged_results if m['folder'] == folder_info['folder_name']), \"ë¯¸ìƒì„±\")\n",
    "    merged_filename = Path(merged_file).name if merged_file != \"ë¯¸ìƒì„±\" else \"ë¯¸ìƒì„±\"\n",
    "\n",
    "    data.append([num_str, entity, expense_type, uploaded, check, missing_str, merged_filename])\n",
    "\n",
    "print(f\"      -> {len(data)-1}ê°œ í–‰ ìƒì„± ì™„ë£Œ\")\n",
    "print()\n",
    "\n",
    "# ê²°ê³¼ ìš”ì•½\n",
    "print(\"=\" * 80)\n",
    "print(\"ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ (ì²˜ìŒ 20ê°œ)\")\n",
    "print(\"=\" * 80)\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "print(df.head(20).to_string(index=False))\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(f\"âœ… ì™„ë£Œ! ì´ {len(all_folders)}ê°œ í´ë” ì²˜ë¦¬, {len(merged_results)}ê°œ í•©ë³¸ ìƒì„±\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# API ì‚¬ìš©ëŸ‰ í‘œì‹œ\n",
    "if Config.USE_GPT_CLASSIFICATION:\n",
    "    print(f\"\\nGPT API í˜¸ì¶œ íšŸìˆ˜: {gpt_extractor.call_count}íšŒ\")\n",
    "    print(f\"ì˜ˆìƒ ë¹„ìš©: ${gpt_extractor.call_count * 0.00005:.4f} (GPT-4o-mini)\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DV3W69VrdRFT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761716241531,
     "user_tz": -540,
     "elapsed": 212040,
     "user": {
      "displayName": "ë³€ë¯¼ìš±(ë³´ëŒ)",
      "userId": "00628206658453677099"
     }
    },
    "outputId": "e2a648c9-ad41-474a-e984-36369be22cc6"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ (ëˆ„ì  ì—…ë°ì´íŠ¸ ë°©ì‹)\n",
    "print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "print(\"â•‘              ğŸ“¤ êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ                              â•‘\")\n",
    "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\")\n",
    "\n",
    "import gspread\n",
    "from google.colab import auth\n",
    "from google.auth import default\n",
    "\n",
    "# Google ì¸ì¦\n",
    "print(\"ğŸ” Google ì¸ì¦ ì¤‘...\")\n",
    "auth.authenticate_user()\n",
    "creds, _ = default()\n",
    "gc = gspread.authorize(creds)\n",
    "print(\"  âœ… ì¸ì¦ ì™„ë£Œ\\n\")\n",
    "\n",
    "# êµ¬ê¸€ ì‹œíŠ¸ ì—´ê¸°\n",
    "print(\"ğŸ“‹ ì‹œíŠ¸ ì—´ê¸° ì¤‘...\")\n",
    "print(f\"  URL: {Config.SHEET_URL}\")\n",
    "spreadsheet = gc.open_by_url(Config.SHEET_URL)\n",
    "worksheet = spreadsheet.get_worksheet(0)\n",
    "print(f\"  âœ… ì‹œíŠ¸ ì—´ê¸° ì™„ë£Œ: {worksheet.title}\\n\")\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„° í™•ì¸ (ì‚­ì œ ì•ˆ í•¨)\n",
    "print(\"ğŸ“Š ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì¤‘...\")\n",
    "existing_data = worksheet.get_all_values()\n",
    "if len(existing_data) > 1:\n",
    "    print(f\"  â„¹ï¸  ê¸°ì¡´ ë°ì´í„°: {len(existing_data)-1}í–‰ (ìœ ì§€ë¨)\")\n",
    "else:\n",
    "    print(f\"  â„¹ï¸  ê¸°ì¡´ ë°ì´í„°: ì—†ìŒ\")\n",
    "\n",
    "# ìƒˆ ë°ì´í„° ì¶”ê°€ (í—¤ë” + ë°ì´í„°)\n",
    "print(f\"\\nğŸ“ ìƒˆ ë°ì´í„° ì—…ë¡œë“œ ì¤‘... ({len(data)}í–‰)\")\n",
    "\n",
    "# í—¤ë”ê°€ ì—†ìœ¼ë©´ ì¶”ê°€\n",
    "if len(existing_data) == 0:\n",
    "    worksheet.update('A1', [data[0]], value_input_option='USER_ENTERED')\n",
    "    start_row = 2\n",
    "else:\n",
    "    start_row = len(existing_data) + 1\n",
    "\n",
    "# ë°ì´í„° ì¶”ê°€ (í—¤ë” ì œì™¸)\n",
    "if len(data) > 1:\n",
    "    worksheet.update(f'A{start_row}', data[1:], value_input_option='USER_ENTERED')\n",
    "    print(f\"  âœ… ì—…ë¡œë“œ ì™„ë£Œ: {start_row}í–‰ë¶€í„° {len(data)-1}ê°œ í–‰ ì¶”ê°€\\n\")\n",
    "else:\n",
    "    print(f\"  âš ï¸  ì¶”ê°€í•  ë°ì´í„° ì—†ìŒ\\n\")\n",
    "\n",
    "# CSV ë°±ì—… ì €ì¥\n",
    "print(\"ğŸ’¾ CSV ë°±ì—… ì €ì¥ ì¤‘...\")\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "output_csv = Path(Config.BASE_PATH) / \"ì£¼ë””ë„¤\" / f\"ì²˜ë¦¬ê²°ê³¼_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "print(f\"  âœ… ë°±ì—… ì™„ë£Œ: {output_csv.name}\\n\")\n",
    "\n",
    "print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "print(\"â•‘              âœ… êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ ì™„ë£Œ!                        â•‘\")\n",
    "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\")\n",
    "print(f\"ğŸ”— ê²°ê³¼ í™•ì¸: {Config.SHEET_URL}\")\n",
    "print(f\"ğŸ“Š ì´ë²ˆ ì„¸ì…˜: {len(data)-1}ê°œ í–‰ ì¶”ê°€\")\n",
    "print(f\"ğŸ“Š ì´ ëˆ„ì : {start_row + len(data) - 2}ê°œ í–‰\")\n",
    "print(f\"ğŸ’¾ ë°±ì—… íŒŒì¼: {output_csv.name}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8o2nFRXyryJ",
    "outputId": "ffc1a9ff-2871-4218-ce87-e7b3c8f6cfaf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1761716412396,
     "user_tz": -540,
     "elapsed": 1298,
     "user": {
      "displayName": "ë³€ë¯¼ìš±(ë³´ëŒ)",
      "userId": "00628206658453677099"
     }
    }
   },
   "execution_count": 30,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}